# ğŸ—ï¸ Arquitectura del Sistema RAG - Taller Solr vs Milvus

## ğŸ“‹ Tabla de Contenidos
1. [VisiÃ³n General](#visiÃ³n-general)
2. [Componentes Principales](#componentes-principales)
3. [Flujo de Datos](#flujo-de-datos)
4. [Detalle TÃ©cnico por Servicio](#detalle-tÃ©cnico-por-servicio)
5. [Pipeline de IndexaciÃ³n](#pipeline-de-indexaciÃ³n)
6. [Pipeline de EvaluaciÃ³n](#pipeline-de-evaluaciÃ³n)
7. [Infraestructura Docker](#infraestructura-docker)
8. [Decisiones de DiseÃ±o](#decisiones-de-diseÃ±o)

---

## ğŸ¯ VisiÃ³n General

Este sistema implementa un **RAG (Retrieval-Augmented Generation)** que compara dos estrategias complementarias de recuperaciÃ³n de documentos:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USUARIO / API CLIENT                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    FastAPI (8000)       â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚           â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
      â”‚ SOLR   â”‚    â”‚  MILVUS   â”‚
      â”‚ (8983) â”‚    â”‚ (19530)   â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”˜    â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
             â”‚      â”‚    â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â–¼â”   â”‚
      â”‚  Corpus      â”‚   â”‚
      â”‚  Indexado    â”‚   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
             BM25    Embeddings
             (lÃ©xico) (semÃ¡ntico)
```

**Dos paradigmas de bÃºsqueda:**
- **Solr (BM25):** RecuperaciÃ³n lÃ©xica basada en frecuencia de tÃ©rminos
- **Milvus (Embeddings):** RecuperaciÃ³n semÃ¡ntica basada en similaridad vectorial

---

## ğŸ§© Componentes Principales

### 1. **API FastAPI** (`services/api/`)
- **Puerto:** 8000
- **Rol:** Punto de entrada unificado
- **Endpoints:**
  - `POST /query_solr` â†’ Consulta Solr
  - `POST /query_milvus` â†’ Consulta Milvus
  - `GET /health` â†’ VerificaciÃ³n de salud

### 2. **Solr** (`services/solr/`)
- **Puerto:** 8983
- **Rol:** Motor de bÃºsqueda lÃ©xica (BM25)
- **Core:** `rag_core`
- **Campo de indexaciÃ³n:** `text_raw`, `lemmas`, `section_title`
- **Volumen:** `./services/solr/data`

### 3. **Milvus** (`services/milvus/`)
- **Puerto:** 19530 (gRPC), 9091 (REST)
- **Rol:** Base de datos vectorial
- **Modelo:** `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- **Dependencias:**
  - **etcd** (2379): CoordinaciÃ³n distribuida
  - **MinIO** (9000): Almacenamiento de objetos (metadatos, logs)
- **VolÃºmenes:** `./services/milvus/data`, `./services/etcd/data`, `./services/minio/data`

### 4. **Indexadores** (`services/indexer/`)
- **index_corpus.py:** Carga el corpus en Solr
- **index_milvus.py:** Vectoriza y carga en Milvus

### 5. **Corpus** (`data/corpus/`)
- **Archivo:** `books_preprocessed_MWE.jsonl`
- **Formato:** JSONL (una lÃ­nea por documento)
- **Campos:** `id`, `section_title`, `text_raw`, `lemmas`

---

## ğŸ”„ Flujo de Datos

### **Fase 1: InicializaciÃ³n (Setup)**
```
setup_rag.sh
    â”œâ”€ Crear estructura de carpetas
    â”œâ”€ Corregir permisos (Solr)
    â”œâ”€ Levantar servicios base
    â”‚   â”œâ”€ Solr
    â”‚   â”œâ”€ etcd
    â”‚   â”œâ”€ MinIO
    â”‚   â””â”€ Milvus
    â”œâ”€ Crear core "rag_core" en Solr
    â”œâ”€ Configurar schema Solr
    â”œâ”€ Indexar corpus en Solr (via indexer)
    â”œâ”€ Vectorizar corpus en Milvus (via indexer_milvus)
    â””â”€ Levantar API FastAPI
```

### **Fase 2: Consulta en Tiempo Real**
```
Cliente HTTP
    â”‚
    â”œâ”€ POST /query_solr {"query": "Â¿quÃ© es X?", "top_k": 10}
    â”‚   â””â”€ Solr BM25
    â”‚       â”œâ”€ TokenizaciÃ³n
    â”‚       â”œâ”€ CÃ¡lculo de TF-IDF
    â”‚       â””â”€ Ranking BM25
    â”‚           â””â”€ Retorna: [doc_id, score, content]
    â”‚
    â”œâ”€ POST /query_milvus {"query": "Â¿quÃ© es X?", "top_k": 10}
    â”‚   â””â”€ Milvus Embeddings
    â”‚       â”œâ”€ Encutar query con modelo
    â”‚       â”œâ”€ BÃºsqueda HNSW/IVF
    â”‚       â””â”€ Ranking por similaridad
    â”‚           â””â”€ Retorna: [doc_id, score, content]
    â”‚
    â””â”€ (Opcional) Procesamiento conjunto
        â””â”€ IntersecciÃ³n / UniÃ³n de resultados
```

### **Fase 3: EvaluaciÃ³n**
```
Queries de prueba (queries_seed.txt)
    â”‚
    â”œâ”€ Gold EstÃ¡ndar DÃ©bil (make_gold_agreement.py)
    â”‚   â”œâ”€ Consultar ambos sistemas (TOP-10)
    â”‚   â”œâ”€ IntersecciÃ³n â†’ Documentos "altamente relevantes"
    â”‚   â””â”€ UniÃ³n - IntersecciÃ³n â†’ Documentos "parcialmente relevantes"
    â”‚       â””â”€ Guardar: gold_weak.jsonl
    â”‚
    â”œâ”€ EvaluaciÃ³n Recall (eval_*_recall.py)
    â”‚   â”œâ”€ Ejecutar cada query contra Solr y Milvus
    â”‚   â””â”€ Calcular: Recall@5, Recall@10
    â”‚
    â”œâ”€ EvaluaciÃ³n ROUGE-L (eval_*_rougeL.py)
    â”‚   â”œâ”€ Recuperar TOP-1 documento
    â”‚   â”œâ”€ Comparar con gold standard
    â”‚   â””â”€ Calcular: ROUGE-L (Precision, Recall, F1)
    â”‚
    â””â”€ EvaluaciÃ³n LLM Judge (eval_llm_judge_*.py)
        â”œâ”€ Recuperar TOP-1 documento
        â”œâ”€ Enviar a Gemini API para evaluaciÃ³n
        â”œâ”€ Calificar: Relevancia, Coherencia, Fidelidad (1-10)
        â””â”€ Guardar resultados en CSV
```

---

## ğŸ”§ Detalle TÃ©cnico por Servicio

### **Solr - Motor de BÃºsqueda LÃ©xica**

#### Schema (ConfiguraciÃ³n de campos)
```json
{
  "fields": [
    {"name": "id", "type": "string", "stored": true, "indexed": true, "required": true},
    {"name": "section_title", "type": "text_general", "stored": true, "indexed": true},
    {"name": "text_raw", "type": "text_general", "stored": true, "indexed": true},
    {"name": "lemmas", "type": "text_general", "stored": true, "indexed": true}
  ]
}
```

#### Algoritmo BM25
```
score(d, q) = Î£(IDF(qi) * (f(qi, d) * (k1 + 1)) / (f(qi, d) + k1 * (1 - b + b * (|d| / avgdl))))

Donde:
- d = documento
- q = query
- f(qi, d) = frecuencia del tÃ©rmino en el documento
- |d| = longitud del documento
- avgdl = longitud promedio de documentos
- k1, b = parÃ¡metros (tÃ­picamente k1=1.2, b=0.75)
```

#### IndexaciÃ³n
```python
# Archivo: services/indexer/index_corpus.py
with open(CORPUS_PATH) as f:
    for line in f:
        doc = json.loads(line)
        payload = {
            "id": doc["id"],
            "section_title": doc["section_title"],
            "text_raw": doc["text_raw"],
            "lemmas": doc["lemmas"]
        }
        requests.post(f"{SOLR_URL}/update", json=[payload])
```

---

### **Milvus - Base de Datos Vectorial**

#### Arquitectura de Dependencias
```
Milvus (Standalone)
â”œâ”€ etcd (CoordinaciÃ³n)
â”‚  â””â”€ Almacena: Metadatos, informaciÃ³n de colecciones
â”œâ”€ MinIO (Object Storage)
â”‚  â””â”€ Almacena: Datos persistentes, logs de inserciÃ³n
â””â”€ RocksDB (Local Storage)
   â””â”€ Almacena: Ãndices, datos temporales
```

#### ConfiguraciÃ³n de ColecciÃ³n
```python
# Archivo: services/indexer/index_milvus.py
collection_schema = CollectionSchema([
    FieldSchema(name="id", dtype=DataType.VARCHAR, max_length=256, is_primary=True),
    FieldSchema(name="section_title", dtype=DataType.VARCHAR, max_length=512),
    FieldSchema(name="text_raw", dtype=DataType.VARCHAR, max_length=4096),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384)
])

# Ãndice HNSW (Hierarchical Navigable Small World)
index_params = {
    "metric_type": "COSINE",
    "index_type": "HNSW",
    "params": {"M": 8, "efConstruction": 200}
}
```

#### Modelo de Embeddings
- **Modelo:** `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- **DimensiÃ³n:** 384 (embedding vector)
- **Idiomas:** MultilingÃ¼e (incluye espaÃ±ol)
- **Ventaja:** Captura similitud semÃ¡ntica, no solo tÃ©rminos exactos

#### BÃºsqueda Vectorial
```python
# 1. Generar embedding de la query
query_embedding = model.encode(query, normalize_embeddings=True)

# 2. BÃºsqueda HNSW
results = collection.search(
    data=[query_embedding],
    anns_field="embedding",
    param={"metric_type": "COSINE", "params": {"ef": 64}},
    limit=10,
    output_fields=["id", "section_title", "text_raw"]
)

# 3. Retornar documentos ordenados por similaridad coseno
```

---

## ğŸ“¥ Pipeline de IndexaciÃ³n

### **Paso 1: PreparaciÃ³n del Corpus**
```
Fuente: data/corpus/books_preprocessed_MWE.jsonl
Formato: 
{
  "id": "book_001_chap_02_sec_03",
  "section_title": "IntroducciÃ³n a la FÃ­sica CuÃ¡ntica",
  "text_raw": "La mecÃ¡nica cuÃ¡ntica estudia el comportamiento...",
  "lemmas": "mecanica quantica estudiar comportamiento..."
}
```

### **Paso 2: IndexaciÃ³n Paralela**

#### **Rama A: IndexaciÃ³n Solr**
```bash
docker compose up --build --exit-code-from indexer indexer

# Internamente:
# 1. Lee CORPUS_PATH lÃ­nea por lÃ­nea
# 2. Extrae campos: id, section_title, text_raw, lemmas
# 3. POST /solr/rag_core/update con JSON array
# 4. Solr procesa:
#    - TokenizaciÃ³n
#    - AnÃ¡lisis de tÃ©rminos
#    - CÃ¡lculo de TF-IDF
#    - ConstrucciÃ³n de Ã­ndice invertido
# 5. Commit: /solr/rag_core/update?commit=true
```

#### **Rama B: IndexaciÃ³n Milvus**
```bash
docker compose up --build --exit-code-from indexer_milvus indexer_milvus

# Internamente:
# 1. Lee CORPUS_PATH lÃ­nea por lÃ­nea
# 2. Para cada documento:
#    a. Carga texto en modelo sentence-transformers
#    b. Genera embedding de 384 dimensiones
#    c. Normaliza embedding (coseno)
# 3. Inserta en Milvus:
#    - Vector embedding
#    - Metadatos: id, section_title, text_raw
# 4. Milvus construye:
#    - Ãndice HNSW (bÃºsqueda jerÃ¡rquica)
#    - Persistencia en MinIO
```

### **Paso 3: VerificaciÃ³n**
```bash
# Solr
curl "http://localhost:8983/solr/rag_core/select?q=*:*&rows=0"
# Retorna: "numFound": 12543

# Milvus
docker compose exec -T milvus python -c \
  "from milvus import connections; 
   connections.connect(host='milvus', port=19530);
   from milvus import Collection;
   c = Collection('documents');
   print(f'Total vectors: {c.num_entities}')"
```

---

## ğŸ“Š Pipeline de EvaluaciÃ³n

### **Arquitectura de Scripts de EvaluaciÃ³n**

```
eval_*.py (6 scripts paralelos)
â”œâ”€ make_gold_agreement.py
â”‚  â”œâ”€ Input: queries_seed.txt
â”‚  â”œâ”€ LÃ³gica: Acuerdo entre Solr + Milvus (TOP-10)
â”‚  â”‚   â”œâ”€ IntersecciÃ³n: highly relevant
â”‚  â”‚   â””â”€ UniÃ³n - Inter: partially relevant
â”‚  â””â”€ Output: gold_weak.jsonl
â”‚
â”œâ”€ eval_solr_recall.py / eval_milvus_recall.py
â”‚  â”œâ”€ MÃ©trica: Recall@5, Recall@10
â”‚  â”œâ”€ CÃ¡lculo: TP / (TP + FN)
â”‚  â”‚   - TP = documentos recuperados en TOP-K que estÃ¡n en gold
â”‚  â”‚   - FN = documentos en gold pero no recuperados
â”‚  â””â”€ Output: metrics_solr_recall.csv, metrics_milvus.csv
â”‚
â”œâ”€ eval_solr_rougeL.py / eval_rougeL.py
â”‚  â”œâ”€ MÃ©trica: ROUGE-L (Longest Common Subsequence)
â”‚  â”œâ”€ CÃ¡lculo: 
â”‚  â”‚   - LCS = subsecuencia comÃºn mÃ¡s larga
â”‚  â”‚   - P = LCS / |retrieved|
â”‚  â”‚   - R = LCS / |gold|
â”‚  â”‚   - F1 = 2PR / (P + R)
â”‚  â””â”€ Output: metrics_solr_rougeL.csv, metrics_rougeL.csv
â”‚
â””â”€ eval_llm_judge_gemini.py / eval_llm_judge_solr.py
   â”œâ”€ Servicio: Google Gemini API
   â”œâ”€ EvaluaciÃ³n cualitativa:
   â”‚   â”œâ”€ Relevancia (1-10): Â¿Responde la pregunta?
   â”‚   â”œâ”€ Coherencia (1-10): Â¿Es el texto coherente?
   â”‚   â””â”€ Fidelidad (1-10): Â¿Es factualmente correcto?
   â””â”€ Output: metrics_llm_judge_*.csv
```

### **MÃ©tricas Detalladas**

#### **Recall@K**
```
DefiniciÃ³n: ProporciÃ³n de documentos relevantes recuperados en TOP-K

Recall@5 = |{documentos relevantes} âˆ© {TOP-5 recuperados}| / |{documentos relevantes}|

Rango: [0, 1]
InterpretaciÃ³n:
  - 1.0 = RecuperÃ³ todos los relevantes en TOP-5
  - 0.5 = RecuperÃ³ 50% de los relevantes
  - 0.0 = No recuperÃ³ ninguno
```

#### **ROUGE-L (F1)**
```
DefiniciÃ³n: MÃ©trica que compara la subsecuencia comÃºn mÃ¡s larga

LCS(ref, hyp) = longest common subsequence length

Precision = LCS / len(hyp)
Recall = LCS / len(ref)
F1 = 2 * P * R / (P + R)

Rango: [0, 1]
Uso: Evaluar similitud entre documento recuperado y referencia
```

#### **LLM Judge (Gemini)**
```
Prompt para cada documento recuperado:

"Given the query: '{query}'
And the retrieved document: '{document}'

Rate on a scale 1-10:
1. Relevancia: Â¿Responde directamente la pregunta?
2. Coherencia: Â¿Es el texto bien estructurado y entendible?
3. Fidelidad: Â¿Es factualmente correcto respecto al corpus?"

Salida: JSON
{
  "relevancia": 8,
  "coherencia": 9,
  "fidelidad": 7
}
```

### **ConsolidaciÃ³n de Resultados**
```python
# exploracion_metricas.ipynb
# Carga todos los CSV y genera comparativas:

# 1. Boxplot ROUGE-L: Solr vs Milvus
# 2. GrÃ¡fico latencia por motor
# 3. Scorecard LLM: Promedios de relevancia/coherencia/fidelidad
# 4. Tabla resumen global

# Salida: resumen_global.csv, PNG comparativos
```

---

## ğŸ³ Infraestructura Docker

### **docker-compose.yml - OrquestaciÃ³n**

```yaml
# Servicios de Infraestructura (Milvus Stack)
etcd        â†’ CoordinaciÃ³n distribuida (2379)
minio       â†’ Object storage (9000, 9001)
minio_setup â†’ InicializaciÃ³n de buckets
milvus      â†’ Base vectorial (19530, 9091)

# Servicios de BÃºsqueda
solr        â†’ Motor lÃ©xico (8983)

# Servicios de AplicaciÃ³n
indexer     â†’ Script de indexaciÃ³n Solr
indexer_milvus â†’ Script de indexaciÃ³n Milvus
api         â†’ FastAPI unificada (8000)

# Red
rag_net (bridge) â†’ ComunicaciÃ³n interna
```

### **Flujo de Dependencias**

```
Inicio:
solr âœ“, etcd âœ“, minio âœ“
         â†“
    minio_setup âœ“
         â†“
      milvus âœ“
    /        \
indexer    indexer_milvus
  â†“            â†“
corpus en   corpus en
 Solr       Milvus
   \         /
    \       /
      api âœ“
```

### **VolÃºmenes Persistentes**

| Servicio | Ruta Host | Ruta Container | PropÃ³sito |
|----------|-----------|-----------------|-----------|
| Solr | `./services/solr/data` | `/var/solr` | Ãndice BM25, configuraciÃ³n |
| Milvus | `./services/milvus/data` | `/var/lib/milvus` | Vectores, metadatos |
| etcd | `./services/etcd/data` | `/etcd-data` | CoordinaciÃ³n distribuida |
| MinIO | `./services/minio/data` | `/data` | Logs, metadatos |
| API | `./services/api` | `/app` | CÃ³digo fuente, datos |
| Indexer | `./services/indexer` | `/app` | Scripts de indexaciÃ³n |
| Corpus | `./data` | `/app/data` | JSONL de documentos |

---

## ğŸ¨ Decisiones de DiseÃ±o

### **1. Dual Retrieval (Solr + Milvus)**

**RazÃ³n:**
- **Solr (BM25):** Efectivo para queries exactas/lÃ©xicas, bajo overhead computacional
- **Milvus (Embeddings):** Captura similitud semÃ¡ntica, maneja variaciones lÃ©xicas

**Beneficio:** ComparaciÃ³n empÃ­rica de paradigmas de recuperaciÃ³n

### **2. Gold EstÃ¡ndar DÃ©bil (Weak Supervision)**

**RazÃ³n:**
- No disponÃ­an de anotadores humanos
- Ambos sistemas son _a priori_ vÃ¡lidos, pero diferentes

**MÃ©todo:**
- **IntersecciÃ³n (Solr âˆ© Milvus):** Alta confianza (ambos coinciden)
- **UniÃ³n - IntersecciÃ³n:** Baja confianza (solo uno lo encuentra)

**LimitaciÃ³n:** Pueden perder documentos relevantes que solo uno de los sistemas recupera

### **3. Modelo MultilingÃ¼e (sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)**

**RazÃ³n:**
- Corpus es en espaÃ±ol
- Modelo preentrenado en 50+ idiomas
- DimensiÃ³n reducida (384) â†’ bajo costo computacional
- Eficiente para dispositivos con recursos limitados

**Alternativas descartadas:**
- BERT/RoBERTa base (no multilingÃ¼e, dimensiÃ³n 768)
- GPT embeddings (API externa, costo monetario)

### **4. Ãndice HNSW en Milvus**

**RazÃ³n:**
- Mejor relaciÃ³n velocidad/precisiÃ³n que IVF
- JerÃ¡rquico (escalable)
- Bajo overhead de memoria

**Alternativas:**
- IVF_FLAT: MÃ¡s rÃ¡pido pero menos preciso
- BRUTE_FORCE: MÃ¡s preciso pero O(n) en bÃºsqueda

### **5. MÃ©tricas MÃºltiples (Recall, ROUGE-L, LLM Judge)**

**RazÃ³n:**
- **Recall:** Mide cobertura cuantitativa
- **ROUGE-L:** Mide similitud textual automÃ¡ticamente
- **LLM Judge:** EvaluaciÃ³n cualitativa humana simulada

**Complementariedad:** Capturan diferentes aspectos de la recuperaciÃ³n

### **6. Pipeline Secuencial pero Modulable**

```
setup_rag.sh
  â”œâ”€ Infraestructura
  â”œâ”€ IndexaciÃ³n Solr
  â”œâ”€ IndexaciÃ³n Milvus
  â””â”€ API

scripts/eval_*.py (independientes)
  â”œâ”€ Pueden correr en paralelo
  â”œâ”€ Salida: CSV individuales
  â””â”€ ConsolidaciÃ³n en notebook
```

**Ventaja:** FÃ¡cil de depurar y reutilizar componentes

---

## ğŸ“ˆ Diagrama de Componentes (C4 Model)

### **Nivel 1: Contexto**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Usuario / Investigador             â”‚
â”‚  (Consulta documento, ejecuta evaluaciones) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Sistema RAG (Contenedor)             â”‚
â”‚  ComparaciÃ³n Solr vs Milvus + EvaluaciÃ³n    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Nivel 2: Contenedor**
```
Sistema RAG
â”œâ”€ API FastAPI
â”‚  â”œâ”€ Query Solr
â”‚  â””â”€ Query Milvus
â”œâ”€ Solr (BM25)
â”‚  â””â”€ Corpus Indexado (lÃ©xico)
â”œâ”€ Milvus (Embeddings)
â”‚  â”œâ”€ etcd
â”‚  â”œâ”€ MinIO
â”‚  â””â”€ Corpus Indexado (vectorial)
â””â”€ EvaluaciÃ³n (scripts)
   â”œâ”€ Gold Generation
   â”œâ”€ Recall
   â”œâ”€ ROUGE-L
   â””â”€ LLM Judge
```

### **Nivel 3: Componentes**
```
[API]â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€[Solr]â”€â”€[Ãndice BM25]
          â”‚
          â””â”€â”€â”€â”€â”€â”€[Milvus]â”€â”¬â”€â”€[etcd]
                          â”œâ”€â”€[MinIO]
                          â””â”€â”€[Ãndice HNSW]

[Corpus]â”€â”€[Indexer Solr]â”€â”€[Solr]
          [Indexer Milvus]â”€[Milvus]

[Queries]â”€[eval_*.py]â”€[Gold/MÃ©tricas]
```

---

## ğŸš€ Flujo de EjecuciÃ³n Completo

```
1. setup_rag.sh
   â”œâ”€ Crear estructura
   â”œâ”€ Levantar Docker Compose
   â”œâ”€ Esperar servicios saludables
   â”œâ”€ Indexar corpus (Solr + Milvus)
   â””â”€ Levantar API

2. make_gold_agreement.py
   â”œâ”€ Leer queries_seed.txt
   â”œâ”€ Consultar Solr + Milvus (TOP-10)
   â”œâ”€ Calcular intersecciÃ³n/uniÃ³n
   â””â”€ Guardar gold_weak.jsonl

3. EvaluaciÃ³n (scripts paralelos)
   â”œâ”€ eval_solr_recall.py â†’ metrics_solr_recall.csv
   â”œâ”€ eval_milvus_recall.py â†’ metrics_milvus.csv
   â”œâ”€ eval_solr_rougeL.py â†’ metrics_solr_rougeL.csv
   â”œâ”€ eval_rougeL.py â†’ metrics_rougeL.csv
   â”œâ”€ eval_llm_judge_solr.py â†’ metrics_llm_judge_solr.csv
   â””â”€ eval_llm_judge_gemini_fixed.py â†’ metrics_llm_judge_gemini_fixed.csv

4. exploracion_metricas.ipynb
   â”œâ”€ Cargar todos los CSV
   â”œâ”€ Generar visualizaciones
   â”œâ”€ Comparativas Solr vs Milvus
   â””â”€ Conclusiones
```

---

## ğŸ“ Estructura de Directorios Completa

```
Taller_RAG/
â”œâ”€â”€ docker-compose.yml          # OrquestaciÃ³n de servicios
â”œâ”€â”€ setup_rag.sh               # Script de inicializaciÃ³n
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ corpus/
â”‚   â”‚   â””â”€â”€ books_preprocessed_MWE.jsonl  # Corpus (12k+ docs)
â”‚   â”œâ”€â”€ queries_seed.txt       # Queries de evaluaciÃ³n
â”‚   â””â”€â”€ gold_weak.jsonl        # Gold estÃ¡ndar (generado)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ make_gold_agreement.py # Genera gold dÃ©bil
â”‚   â”œâ”€â”€ eval_*.py              # 6 scripts de evaluaciÃ³n
â”‚   â”œâ”€â”€ exploracion_metricas.ipynb  # AnÃ¡lisis y visualizaciÃ³n
â”‚   â””â”€â”€ eval_log.txt           # Log de ejecuciÃ³n
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ Dockerfile         # Image FastAPI
â”‚   â”‚   â”œâ”€â”€ main.py           # CÃ³digo de la API
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ data/
â”‚   â”‚
â”‚   â”œâ”€â”€ indexer/
â”‚   â”‚   â”œâ”€â”€ Dockerfile         # Image indexaciÃ³n
â”‚   â”‚   â”œâ”€â”€ index_corpus.py   # IndexaciÃ³n Solr
â”‚   â”‚   â”œâ”€â”€ index_milvus.py   # IndexaciÃ³n Milvus
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ data/
â”‚   â”‚
â”‚   â”œâ”€â”€ solr/
â”‚   â”‚   â””â”€â”€ data/              # Volumen persistente Solr
â”‚   â”‚
â”‚   â”œâ”€â”€ milvus/
â”‚   â”‚   â”œâ”€â”€ data/              # Volumen persistente Milvus
â”‚   â”‚   â”œâ”€â”€ etcd/
â”‚   â”‚   â””â”€â”€ minio/
â”‚   â”‚
â”‚   â”œâ”€â”€ etcd/
â”‚   â”‚   â””â”€â”€ data/              # Volumen persistente etcd
â”‚   â”‚
â”‚   â””â”€â”€ minio/
â”‚       â””â”€â”€ data/              # Volumen persistente MinIO
â”‚
â””â”€â”€ reports/
    â”œâ”€â”€ metrics_*.csv          # Resultados de evaluaciÃ³n
    â”œâ”€â”€ resumen_global.csv     # Consolidado
    â””â”€â”€ *.png                  # GrÃ¡ficos comparativos
```

---

## ğŸ” Puntos Clave de IntegraciÃ³n

### **API - Solr**
```
POST /query_solr
Request: {"query": str, "top_k": int}
Response: [{"id": str, "score": float, "content": str}, ...]
ConexiÃ³n: HTTP REST a http://solr:8983/solr/rag_core
```

### **API - Milvus**
```
POST /query_milvus
Request: {"query": str, "top_k": int}
Response: [{"id": str, "score": float, "content": str}, ...]
ConexiÃ³n: gRPC a milvus:19530
```

### **IndexaciÃ³n - Corpus**
```
Fuente: ./data/corpus/books_preprocessed_MWE.jsonl
Lectura: JSONL lÃ­nea por lÃ­nea
Destino Solr: POST /solr/rag_core/update
Destino Milvus: Vector DB + Metadata storage
```

---

## ğŸ“ ConclusiÃ³n ArquitectÃ³nica

Este sistema es una **comparaciÃ³n empÃ­rica rigurosa** entre dos paradigmas de recuperaciÃ³n de informaciÃ³n:

| Aspecto | Solr (BM25) | Milvus (Embeddings) |
|---------|-------------|-------------------|
| **Enfoque** | LÃ©xico (tÃ©rminos exactos) | SemÃ¡ntico (significado) |
| **Algoritmo** | TF-IDF + BM25 | Embeddings + HNSW |
| **Overhead** | Bajo | Medio (GPU opcional) |
| **PrecisiÃ³n lÃ©xica** | Alta | Media |
| **Captura semÃ¡ntica** | Baja | Alta |
| **Escalabilidad** | Muy buena | Buena |

**ConclusiÃ³n esperada:** En tareas de preguntas abiertas, Milvus capturarÃ¡ mejor la similitud semÃ¡ntica, mientras que Solr serÃ¡ mÃ¡s preciso en queries estructuradas.

